{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "[0]_BuildingDamage_STA221_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/escaduto/BuildingExtraction/blob/master/%5B0%5D_BuildingDamage_STA221_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDfxMXly7XBx",
        "colab_type": "text"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSulvARN1CeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "31bf92ba-7db8-476a-fd29-c6e661216b8c"
      },
      "source": [
        "from google.colab import drive # import drive from google colab\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JZM0eF81J_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf4ebb41-b60d-4d95-bc78-3169e495ebe5"
      },
      "source": [
        "%cd \"/content/drive/My Drive/STA221_FinalProj\"\n",
        "rootPath = r'/content/drive/My Drive/STA221_FinalProj/Data/FireDataset'\n",
        "os.chdir(rootPath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1xQURupjEB6eidd-IqFW8qhj4FjXXQz8r/STA221_FinalProj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBOHu6AH1d6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy7k9Pqt1jq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install simplejson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFUyDAEu09AN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime, date, time, timedelta\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from shapely import wkt\n",
        "from pandas.io.json import json_normalize\n",
        "import geopandas as gpd\n",
        "from sys import argv\n",
        "from os.path import exists\n",
        "import simplejson as json "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GF6Im3U09AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstFiles(rootPath, ext):\n",
        "  '''\n",
        "  get list of files based on directory and extension inputs \n",
        "  '''\n",
        "  emptyList = []\n",
        "  root = rootPath\n",
        "  for path, subdirs, files in os.walk(root):\n",
        "      for names in files: \n",
        "          if names.endswith(ext) and not names.startswith(\"._\"):\n",
        "              emptyList.append(path + '/' + names)\n",
        "  return(emptyList)\n",
        "\n",
        "def createFolder(rootPath, folderName): \n",
        "  '''\n",
        "  Create new folder in root path \n",
        "  '''\n",
        "  folderPath = os.path.join(rootPath, folderName) \n",
        "  if not os.path.exists(folderPath):\n",
        "      os.makedirs(folderPath)\n",
        "  return folderPath + \"/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFXLqJv57hUy",
        "colab_type": "text"
      },
      "source": [
        "## Unzip .tar\n",
        "\n",
        "Dataset from xview challenge can be obtained here: https://xview2.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBsLAT4L09BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainData = 'train_images_labels_targets.tar'\n",
        "testData = 'test_images_labels_targets.tar'\n",
        "holdData = 'hold_images_labels_targets.tar'\n",
        "#tier3Data = 'tier3.tar'\n",
        "\n",
        "tar_files = [(trainData, 'train_all'), (testData, 'test_all'), (holdData, 'hold_all')]\n",
        "\n",
        "for tar in tar_files: \n",
        "  tf = tarfile.open(os.path.join('TarFiles', tar[0]))\n",
        "  outpath = createFolder(rootPath, tar[1])\n",
        "  tf.extractall(outpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZlGa3WN7poD",
        "colab_type": "text"
      },
      "source": [
        "## Read-in data into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdhAQfzV09BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDataInfo(rootPath, folderName):\n",
        "  '''\n",
        "  Get info of data from original XView challenge dataset (.json)\n",
        "  extracting attributes from metadata: location_name, ID, disaster type, \n",
        "  img_date, pre_post_disaster, and img_name to create dataframe\n",
        "  '''\n",
        "  json_files = lstFiles(os.path.join(rootPath, folderName), '.json')\n",
        "\n",
        "  locationName = []\n",
        "  disasterType = []  \n",
        "  ID = [] \n",
        "  pre_post = [] \n",
        "  date = [] \n",
        "  img_name = [] \n",
        "\n",
        "  for jsn in json_files: \n",
        "      nm = jsn.split(\"\\\\\")\n",
        "      fileName = nm[-1]\n",
        "      nmm = fileName.split(\"_\")\n",
        "      ID.append(str(nmm[1]))\n",
        "      pre_post.append(nmm[2])\n",
        "      img_name.append(fileName[:-5])\n",
        "      data = json.load(open(jsn))\n",
        "      disasterType.append(data['metadata']['disaster_type'])\n",
        "      date.append(data['metadata']['capture_date'])\n",
        "      locationName.append(data['metadata']['disaster'])\n",
        "\n",
        "  dataInfo = pd.DataFrame({ 'location_name': locationName, 'ID':ID, 'disaster_type': disasterType, \n",
        "                            'img_date': date, 'pre_post_disaster': pre_post, 'img_name':img_name})\n",
        "  return dataInfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtwfKMxO09Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_firesDF = getDataInfo(rootPath, 'test_all')\n",
        "hold_firesDF = getDataInfo(rootPath, 'hold_all')\n",
        "train_firesDF = getDataInfo(rootPath, 'train_all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tUse6Ei7vz8",
        "colab_type": "text"
      },
      "source": [
        "## Filter Data & Move "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4QwRcBI09Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "def moveFiles(rootPath, inputFolder, folderDict, outputFolder, dataDF):\n",
        "  '''\n",
        "  Based on dataframe with disaster type info, move files into new folder\n",
        "  if only it is fire. \n",
        "  '''\n",
        "  filterDF = dataDF[dataDF['disaster_type'] == 'fire']\n",
        "  for index, row in filterDF.iterrows():\n",
        "      for dic in folderDict: \n",
        "          flPath = os.path.join(rootPath, inputFolder, dic[0])\n",
        "          src = os.path.join(flPath, row.img_name + dic[1])\n",
        "          mainFolder = createFolder(os.path.join(rootPath, 'Data'), outputFolder)\n",
        "          eventFolder = createFolder(mainFolder, row.location_name)\n",
        "          IDFolder = createFolder(eventFolder, row.ID)\n",
        "          dst = os.path.join(IDFolder, row.img_name + dic[1])\n",
        "          copyfile(src, dst)\n",
        "\n",
        "folderExt = [['images', '.png'], ['labels', '.json'], ['targets', '_target.png']]\n",
        "\n",
        "moveFiles(rootPath, 'test_all', folderExt, 'test', test_firesDF)     \n",
        "moveFiles(rootPath, 'hold_all', folderExt,'hold', hold_firesDF)     \n",
        "moveFiles(rootPath, 'train_all', {'images' : 'png', 'labels' : '.json', 'targets': '_target.png'}, 'train', train_firesDF)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNO4ub7170qp",
        "colab_type": "text"
      },
      "source": [
        "## Convert to Readable Format (i.e. GEOJSON)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-tr-f3009Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ioff()\n",
        "\n",
        "def createGeoFiles(fullDF, inputFolder, outputFolder):\n",
        "  '''\n",
        "  Based on the dataframe, if disaster_type is fire then read .json file as geodataframe\n",
        "  Add in damage attribute 'no-damage' for pre disaster events. \n",
        "  Create folder and event folders to organize each instance based on unique IDs\n",
        "  Convert to geojson, shpfile, .png files \n",
        "  '''\n",
        "  filterDF = fullDF[fullDF['disaster_type'] == 'fire']\n",
        "  for index, row in filterDF.iterrows():\n",
        "      try:\n",
        "          pth = os.path.join(rootPath, inputFolder, row.location_name, row.ID)\n",
        "          jsonFile = os.path.join(pth, row.img_name + '.json')\n",
        "          data = json.load(open(jsonFile)) # read as json file \n",
        "          df = json_normalize(data['features']['lng_lat']) # convert to df\n",
        "          df['wkt'] = df['wkt'].apply(wkt.loads)\n",
        "          gdf = gpd.GeoDataFrame(df, geometry='wkt') # read as geodf\n",
        "          gdf['date'] = row.img_date\n",
        "          gdf['pre_post_disaster'] = row.pre_post_disaster\n",
        "          gdf['disaster_type'] = row.disaster_type\n",
        "          gdf['location_name'] = row.location_name\n",
        "          gdf['ID'] = row.ID\n",
        "          if row.pre_post_disaster == 'pre': \n",
        "              gdf['damage'] = 'no-damage'\n",
        "          else: \n",
        "              gdf['damage'] = df['properties.subtype']\n",
        "\n",
        "          mainFolder = createFolder(rootPath, outputFolder)\n",
        "          eventFolder = createFolder(mainFolder, row.location_name)\n",
        "          IDFolder = createFolder(eventFolder, row.ID)\n",
        "          gdf.crs = {'init' :'epsg:4326'}\n",
        "          gdf.to_file(os.path.join(IDFolder, row.img_name + '.geojson'), driver='GeoJSON')\n",
        "          shpPath = createFolder(IDFolder, row.img_name)\n",
        "          gdf.to_file(os.path.join(shpPath, row.img_name + '.shp'))\n",
        "          gdf.plot(column='damage', cmap='PiYG')\n",
        "          plt.savefig(os.path.join(IDFolder, row.pre_post_disaster + \"_\" + row.location_name + '.png'))\n",
        "          plt.close()\n",
        "      except: \n",
        "          continue        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9cvCvqH09Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert .json to geojson\n",
        "createGeoFiles(train_firesDF, 'train', 'train_geo')\n",
        "createGeoFiles(test_firesDF, 'test', 'test_geo')\n",
        "createGeoFiles(hold_firesDF, 'hold', 'hold_geo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2mriIjh76Db",
        "colab_type": "text"
      },
      "source": [
        "## Merge Training/Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gkwtE209Bx",
        "colab_type": "code",
        "colab": {},
        "outputId": "b6188841-cf23-4016-bddd-86f8859e1de0"
      },
      "source": [
        "import glob\n",
        "# get all shapefiles in train folder \n",
        "# merge into one \n",
        "test_files = lstFiles('test_geo', '.shp')\n",
        "test_files = lstFiles('train_geo', '.shp')\n",
        "hold_files = lstFiles('hold_geo', '.shp')\n",
        "\n",
        "# merge with geopandas instead \n",
        "#arcpy.Merge_management(shp_files, os.path.join(rootPath, 'test_building.shp')) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['F:\\\\BuildingChallenge\\\\Data\\\\test_geo\\\\santa-rosa-wildfire\\\\00000000\\\\santa-rosa-wildfire_00000000_post_disaster\\\\santa-rosa-wildfire_00000000_post_disaster.shp',\n",
              " 'F:\\\\BuildingChallenge\\\\Data\\\\test_geo\\\\santa-rosa-wildfire\\\\00000000\\\\santa-rosa-wildfire_00000000_pre_disaster\\\\santa-rosa-wildfire_00000000_pre_disaster.shp',\n",
              " 'F:\\\\BuildingChallenge\\\\Data\\\\test_geo\\\\santa-rosa-wildfire\\\\00000001\\\\santa-rosa-wildfire_00000001_post_disaster\\\\santa-rosa-wildfire_00000001_post_disaster.shp',\n",
              " 'F:\\\\BuildingChallenge\\\\Data\\\\test_geo\\\\santa-rosa-wildfire\\\\00000001\\\\santa-rosa-wildfire_00000001_pre_disaster\\\\santa-rosa-wildfire_00000001_pre_disaster.shp',\n",
              " 'F:\\\\BuildingChallenge\\\\Data\\\\test_geo\\\\santa-rosa-wildfire\\\\00000012\\\\santa-rosa-wildfire_00000012_post_disaster\\\\santa-rosa-wildfire_00000012_post_disaster.shp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}